# -*- coding: utf-8 -*-
"""regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F1afesIPNSiBUjvn2c-BEu7rQaF45Ycf
"""
import argparse


def main():
    parser = argparse.ArgumentParser(description="Training a classification model")
    parser.add_argument("--model_path", required=True)
    parser.add_argument("--dataset_path", required=True)
    parser.add_argument("--val_dataset_path", required=True)
    args = parser.parse_args()
    print(f"Training a classification model. Output will be saved at {args.model_path}. Dataset will be loaded from {args.dataset_path}. Validation dataset will be loaded from {args.val_dataset_path}.")


if __name__=="main_":
    main()

import os
import sys
import gzip
#!pip install torch_geometric
import torch
from torch_geometric.data import Dataset, Data
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import NNConv, global_mean_pool, global_add_pool,GATConv
from torch_geometric.nn import ChebConv
from torch_geometric.data import Dataset
# from torch_geometric.data import DataLoader, Dataset
from torch_geometric.loader import DataLoader
import time
from sklearn.model_selection import KFold
from sklearn.metrics import roc_auc_score
import matplotlib.pyplot as plt
import numpy as np
import networkx as nx
from torch.optim.lr_scheduler import StepLR,MultiStepLR


modelpath=sys.argv[2]
traindatasetpath=sys.argv[4]
validdatasetpath=sys.argv[6]

trainpath=traindatasetpath
validpath=validdatasetpath

import torch
import gzip
from torch_geometric.data import Data, Dataset, DataLoader, Batch

class MyDataset:
    def __init__(self, path):
        # Load gzipped data
        self.f1 = path + '/node_features.csv.gz'
        self.f2 = path + '/num_nodes.csv.gz'
        self.f3 = path + '/num_edges.csv.gz'
        self.f4 = path + '/edges.csv.gz'
        self.f5 = path + '/graph_labels.csv.gz'
        self.f6 = path + '/edge_features.csv.gz'
        self.graph_label = []
        self.data_list = []
        self.edge = []
        self.node_features = []
        self.edge_features = []
        self.node_feat = []
        self.edge_feat = []
        self.edges = [[], []]
        self.data_list = []
        self.graph_label = []
        self.process_csv(self.f1, self.f2, self.f3, self.f4, self.f5)

    def __len__(self):
        return len(self.data_list)

    def __getitem__(self, idx):
        return self.data_list[idx]

    def process_csv(self, f1, f2, f3, f4, f5):
        with gzip.open(self.f5, 'rt') as file:
            for line in file:
                # line = line[:-1]
                if 'nan' not in line:
                    self.graph_label.append([(float(line))])
                else:
                    self.graph_label.append([int(1)])

        self.graph_label = torch.tensor(self.graph_label)

        with gzip.open(self.f4, 'rt') as file:
            for line in file:
                self.edges[0].append(list(map(int, line.split(',')))[0])
                self.edges[1].append(list(map(int, line.split(',')))[1])

        with gzip.open(self.f3, 'rt') as file:
            c = 0
            for k, line in enumerate(file):
                t = []
                line = line[:-1]
                t.append(self.edges[0][c:int(float(line)) + c])
                t.append(self.edges[1][c:int(float(line)) + c])
                self.edge.append(torch.tensor(t))
                c += int(float(line))

        with gzip.open(self.f1, 'rt') as file:
            for line in file:
                self.node_feat.append(list(map(float, line.split(','))))

        self.node_feat = torch.tensor(self.node_feat)

        with gzip.open(self.f6, 'rt') as file:
            for line in file:
                self.edge_feat.append(list(map(float, line.split(','))))

        self.edge_feat = torch.tensor(self.edge_feat)

        with gzip.open(self.f2, 'rt') as file:
            c = 0
            for line in file:
                self.node_features.append(self.node_feat[c:int(float(line)) + c])
                c += int(line)

        with gzip.open(self.f3, 'rt') as file:
            c = 0
            for line in file:
                self.edge_features.append(self.edge_feat[c:int(float(line)) + c])
                c += int(line)

        l = len(self.graph_label)
        for i in range(l):
            if len(self.edge[i][0]) > 0:
                data = Data(x=self.node_features[i], edge_index=self.edge[i], y=self.graph_label[i],
                            edge_attr=self.edge_features[i])
                self.data_list.append(data)

        return self.data_list


# Instantiate the dataset
# dataset = MyDataset(trainpath)

# Create a PyTorch DataLoader

train=MyDataset(trainpath)
validate=MyDataset(validpath)



class RegressionModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, dropout_prob, num_heads):
        super(RegressionModel, self).__init__()

        self.node_encoder = nn.Sequential(
            GATConv(input_dim, hidden_dim, heads=num_heads),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_dim),
            nn.Dropout(p=dropout_prob),
            GATConv(hidden_dim * num_heads, hidden_dim, heads=num_heads),
            nn.ReLU(),
            nn.BatchNorm1d(hidden_dim),
        )

        self.mlp = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            # nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            # nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            # nn.ReLU(),
            nn.Linear(hidden_dim, output_dim),
        )

    def forward(self, data):
        x, edge_index, _ = data.x, data.edge_index, data.edge_attr

        # Manually apply GAT layers in sequence
        for layer in self.node_encoder:
            if isinstance(layer, GATConv):
                x = layer(x, edge_index)
            else:
                x = layer(x)

        x = global_add_pool(x, data.batch)

        x = self.mlp(x)

        return x

from sklearn.model_selection import train_test_split

learning_rate=0.05
dout=0.2
batch_size=10
learning_rate=0.01
num_epochs=50
best_roc_auc = 0.0
best_model = None
num_folds=4
input_dim=train[0].x.shape[1]
hidden_dim=9
output_dim=1
dropout_prob=0.2
num_heads=1
model = RegressionModel(input_dim, hidden_dim, output_dim, dropout_prob,num_heads)

optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
scheduler = MultiStepLR(optimizer, milestones=[5, 10, 15], gamma=0.5)
criterion = nn.MSELoss()


all_indices = list(range(len(train)))

# Split the indices into 80% training and 20% validation

def custom_collate(batch):
        return Batch.from_data_list(batch)

validation_data = DataLoader(validate, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)
import math
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
best_model=None
overall_rmse=10000
valid_losses_all_epoch = []
train_loss_epoch_good=[]
for fold in range (num_folds):
    all_indices = list(range(len(train)))
    train_idx, val_idx = train_test_split(all_indices, test_size=0.2, random_state=32)
    # print(f"Fold {fold + 1}/{num_folds}")
    train_data = [train[i] for i in train_idx]
    val_data = [train[i] for i in val_idx]
    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=custom_collate)
    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=custom_collate)

    model = RegressionModel(input_dim, hidden_dim, output_dim, dropout_prob,num_heads)

    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    scheduler = MultiStepLR(optimizer, milestones=[5, 10, 15], gamma=0.5)
    criterion = nn.MSELoss()
    train_one_epoch=[]
    valid_loss=[]
    for epoch in range(num_epochs):
        print(f"epoch : {epoch}")
        model.train()
        train_loss = 0.0
        for data in train_loader:
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, data.y)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()
        train_one_epoch.append(math.sqrt(train_loss / len(train_loader)))
        # print(f"loss : {train_loss / len(train_loader)}")
        model.eval()
        with torch.no_grad():
            rms_val_loss=0
            for data in validation_data:
                outputs = model(data)
                rms_val_loss=rms_val_loss + (criterion(outputs, data.y))
            rms_val_loss=torch.sqrt(rms_val_loss/len(validation_data))
            valid_loss.append(rms_val_loss)
            # print(f"rms : {rms_val_loss}")
            # print(f"rms loss : {new_rms}")
            if(rms_val_loss<overall_rmse):
              print("best model updated")
              overall_rmse=rms_val_loss
              best_model=model
              train_loss_epoch_good=train_one_epoch
              valid_losses_all_epoch=valid_loss

plt.plot(range(1, num_epochs + 1), train_loss_epoch_good, label=f' Train Loss')
plt.plot(range(1, num_epochs + 1), valid_losses_all_epoch, label=f' Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training and Validation Losses')
plt.legend()
plt.show()
plt.savefig('regression_train_valid_loss.png')

torch.save(best_model.state_dict(), modelpath)

